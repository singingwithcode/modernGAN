{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DCGAN with CIFTAR10"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-24 19:29:07.694169: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.utils as vutils\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import inception_v3\n",
        "from scipy import linalg\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "manual_seed = 42\n",
        "torch.manual_seed(manual_seed)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "nz = 100 # Size of latent vector\n",
        "ngf = 64 # Size of generator feature maps\n",
        "ndf = 64 # Size of discriminator feature maps\n",
        "nc = 3\n",
        "learning_rate = 0.0002\n",
        "num_epochs = 2\n",
        "lr = 0.0002 # Learning rate\n",
        "beta1 = 0.5 # Adam optimizer beta1\n",
        "fid_sample_size = 10000 # Number of samples to compute FID score\n",
        "fid_batch_size = 50 # Batch size for computing FID score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "## Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "transform = transforms.Compose([transforms.Resize(64), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_dataset = dsets.CIFAR10(root='./data', train=True, download=True, transform=transform)    #tf.keras.datasets.cifar10.load_data()\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "THY-sZMiQ4UV"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-tEyxE-GMC48"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6bpTcDqoLWjY"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=100, img_channels=3, output_size=32):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.fc = nn.Linear(latent_dim, 512 * 4 * 4, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(512)\n",
        "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(256)\n",
        "        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.deconv3 = nn.ConvTranspose2d(128, img_channels, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.deconv4 = nn.ConvTranspose2d(img_channels, img_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.fc(z)\n",
        "        x = x.view(-1, 512, 4, 4)\n",
        "        x = F.relu(self.bn1(x))\n",
        "        x = F.relu(self.bn2(self.deconv1(x)))\n",
        "        x = F.relu(self.bn3(self.deconv2(x)))\n",
        "        x = torch.tanh(self.deconv3(x))\n",
        "        x = self.deconv4(x)\n",
        "        x = F.interpolate(x, size=self.output_size, mode='bilinear', align_corners=True)\n",
        "        return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D0IKnaCtg6WE"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dw2tPLmk2pEP"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(img_channels, 64, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.conv5 = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv1(x), negative_slope=0.2)\n",
        "        x = F.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.2)\n",
        "        x = F.leaky_relu(self.bn3(self.conv3(x)), negative_slope=0.2)\n",
        "        x = F.leaky_relu(self.bn4(self.conv4(x)), negative_slope=0.2)\n",
        "        x = self.conv5(x)\n",
        "        return x.view(1, -1)    #x.view(-1, 1)    #(x.size(0), -1)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "## Helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_fid(real_images, generated_images, device, batch_size=50, dims=2048):\n",
        "    \"\"\"\n",
        "    Calculates the Fr√©chet Inception Distance (FID) between the real images and the generated images.\n",
        "    \"\"\"\n",
        "    # Load the Inception v3 model pre-trained on ImageNet\n",
        "    inception_model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    inception_model.eval()\n",
        "    \n",
        "    # Calculate the mean and covariance matrix of the real images\n",
        "    real_images = np.transpose(real_images, (0, 3, 1, 2))\n",
        "    mu_real, sigma_real = calculate_activation_statistics(real_images, inception_model, device, batch_size, dims)\n",
        "    \n",
        "    # Calculate the mean and covariance matrix of the generated images\n",
        "    generated_images = np.transpose(generated_images, (0, 3, 1, 2))\n",
        "    mu_gen, sigma_gen = calculate_activation_statistics(generated_images, inception_model, device, batch_size, dims)\n",
        "    \n",
        "    # Calculate the FID between the real and generated images\n",
        "    fid_score = calculate_frechet_distance(mu_real, sigma_real, mu_gen, sigma_gen)\n",
        "    \n",
        "    return fid_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_inception_score(images, net_inception, batch_size, splits):\n",
        "    # Get predictions for each batch of images\n",
        "    preds = []\n",
        "    num_batches = int(math.ceil(float(images.shape[0]) / float(batch_size)))\n",
        "    for i in range(num_batches):\n",
        "        batch = images[i*batch_size:(i+1)*batch_size]\n",
        "        with torch.no_grad():\n",
        "            batch = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=True)\n",
        "            pred = F.softmax(net_inception(batch), dim=1).cpu().numpy()\n",
        "        preds.append(pred)\n",
        "\n",
        "    # Calculate Inception score\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    scores = []\n",
        "    for i in range(splits):\n",
        "        part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n",
        "        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
        "        kl = np.mean(np.sum(kl, 1))\n",
        "        scores.append(np.exp(kl))\n",
        "\n",
        "    return np.mean(scores), np.std(scores)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(netG, netD, train_loader, num_epochs, optimizerG, optimizerD, criterion, device):\n",
        "    # Lists to keep track of losses over time\n",
        "    G_losses = []\n",
        "    D_losses = []\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop over batches in the dataset\n",
        "        for i, (real_images, _) in enumerate(train_loader):\n",
        "            real_images = real_images.to(device)\n",
        "\n",
        "            # Update discriminator network\n",
        "            netD.zero_grad()\n",
        "            real_labels = torch.full((real_images.size(0),), 1, device=device)\n",
        "            fake_labels = torch.full((real_images.size(0),), 0, device=device)\n",
        "            fake_labels = fake_labels.unsqueeze(1)\n",
        "            real_labels = real_labels.unsqueeze(1) #added\n",
        "            real_labels.flatten()\n",
        " \n",
        "            # Train discriminator on real images\n",
        "            real_output = netD(real_images)\n",
        "            real_loss = criterion(real_output, real_labels)\n",
        "            real_loss.backward()\n",
        "\n",
        "            # Train discriminator on fake images\n",
        "            noise = torch.randn(real_images.size(0), 100, 1, 1, device=device)\n",
        "            fake_images = netG(noise)\n",
        "            fake_output = netD(fake_images.detach())\n",
        "            fake_loss = criterion(fake_output, fake_labels)\n",
        "            fake_loss.backward()\n",
        "\n",
        "            # Update discriminator parameters\n",
        "            optimizerD.step()\n",
        "\n",
        "            # Update generator network\n",
        "            netG.zero_grad()\n",
        "            noise = torch.randn(real_images.size(0), 100, 1, 1, device=device)\n",
        "            fake_images = netG(noise)\n",
        "            output = netD(fake_images)\n",
        "            loss = criterion(output, real_labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Update generator parameters\n",
        "            optimizerG.step()\n",
        "\n",
        "            # Keep track of losses\n",
        "            G_losses.append(loss.item())\n",
        "            D_losses.append(real_loss.item() + fake_loss.item())\n",
        "\n",
        "        # Print progress\n",
        "        print('Epoch [{}/{}], Step [{}/{}], D_loss: {:.4f}, G_loss: {:.4f}'\n",
        "              .format(epoch + 1, num_epochs, i + 1, len(train_loader), D_losses[-1], G_losses[-1]))\n",
        "\n",
        "    return G_losses, D_losses\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_images(netG, num_images, device):\n",
        "    # Generate noise\n",
        "    noise = torch.randn(num_images, 100, 1, 1, device=device)\n",
        "\n",
        "    # Generate fake images\n",
        "    fake_images = netG(noise).detach().cpu()\n",
        "\n",
        "    # Rescale pixel values from [-1, 1] to [0, 1]\n",
        "    fake_images = (fake_images + 1) / 2\n",
        "\n",
        "    # Plot images\n",
        "    fig, ax = plt.subplots(1, num_images, figsize=(20, 5))\n",
        "    for i in range(num_images):\n",
        "        ax[i].imshow(np.transpose(fake_images[i], (1, 2, 0)))\n",
        "        ax[i].axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_images(G_losses, D_losses, fake_images, real_images):\n",
        "    # Plot losses over time\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "    plt.plot(G_losses, label=\"Generator\")\n",
        "    plt.plot(D_losses, label=\"Discriminator\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot real and fake images\n",
        "    fig, ax = plt.subplots(1, 12, figsize=(20, 5))\n",
        "    for i in range(12):\n",
        "        ax[i].imshow(np.transpose(real_images[i], (1, 2, 0)))\n",
        "        ax[i].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 12, figsize=(20, 5))\n",
        "    for i in range(12):\n",
        "        ax[i].imshow(np.transpose(fake_images[i], (1, 2, 0)))\n",
        "        ax[i].axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    random.seed(1234)\n",
        "    torch.manual_seed(1234)\n",
        "\n",
        "    # Set device (CPU or GPU)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Create generator and discriminator networks\n",
        "    netG = Generator(nz, ngf, nc).to(device)\n",
        "    netD = Discriminator(nc).to(device)\n",
        "\n",
        "    # Initialize weights of neural networks\n",
        "    netG.apply(weights_init)\n",
        "    netD.apply(weights_init)\n",
        "\n",
        "    # Set loss function and optimizer\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
        "    optimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
        "\n",
        "    # Train the DCGAN\n",
        "    G_losses, D_losses = train(netG, netD, train_loader, num_epochs, optimizerG, optimizerD, criterion, device)\n",
        "\n",
        "    # Generate sample images and plot them\n",
        "    generated_images = generate_images(netG, device)\n",
        "    plot_images(generated_images)\n",
        "\n",
        "    # Calculate FID and Inception Score of generated images\n",
        "    fid_score = calculate_fid(test_loader.dataset.data[:10000], generated_images, device)\n",
        "    is_score = calculate_inception_score(generated_images, device)\n",
        "\n",
        "    # Print FID and Inception Score\n",
        "    print('FID score:', fid_score)\n",
        "    print('Inception Score:', is_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.3639, -0.5641, -0.2319, -0.0533, -0.8460, -0.2053, -1.2609, -0.4069,\n",
            "         -1.4518, -1.0236,  0.1786, -0.6088,  0.5581, -3.5042,  0.8576, -0.0637,\n",
            "          0.7037, -0.4831, -1.2059, -1.0361, -0.7458, -0.7315, -1.2684, -2.3548,\n",
            "          0.6333,  0.1499,  0.5496, -0.4615,  0.7192,  0.3913, -1.5533, -1.1736,\n",
            "         -2.4295, -0.5262, -0.4120, -2.0019,  0.8015, -0.3588, -0.4224, -0.9425,\n",
            "         -1.5760, -0.6769, -0.6101, -0.7147, -0.8330, -1.1415, -2.4340, -1.4335,\n",
            "          0.8336, -0.8447,  1.0018, -1.3018, -0.3290,  0.7490, -1.7046, -1.8792,\n",
            "          1.1098,  0.0252, -1.1534, -0.8696,  0.2536, -1.1089, -0.5838,  0.4491]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "tensor([[1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1]])\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main()\n",
            "Cell \u001b[0;32mIn[12], line 23\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizerD \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(netD\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate, betas\u001b[39m=\u001b[39m(beta1, \u001b[39m0.999\u001b[39m))\n\u001b[1;32m     22\u001b[0m \u001b[39m# Train the DCGAN\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m G_losses, D_losses \u001b[39m=\u001b[39m train(netG, netD, train_loader, num_epochs, optimizerG, optimizerD, criterion, device)\n\u001b[1;32m     25\u001b[0m \u001b[39m# Generate sample images and plot them\u001b[39;00m\n\u001b[1;32m     26\u001b[0m generated_images \u001b[39m=\u001b[39m generate_images(netG, device)\n",
            "Cell \u001b[0;32mIn[9], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(netG, netD, train_loader, num_epochs, optimizerG, optimizerD, criterion, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(real_output)\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(real_labels)\n\u001b[0;32m---> 24\u001b[0m real_loss \u001b[39m=\u001b[39m criterion(real_output, real_labels)\n\u001b[1;32m     25\u001b[0m real_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m \u001b[39m# Train discriminator on fake images\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.11/site-packages/torch/nn/modules/loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.11/site-packages/torch/nn/functional.py:3089\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3087\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3088\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[0;32m-> 3089\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3092\u001b[0m     )\n\u001b[1;32m   3094\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3095\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
            "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size."
          ]
        }
      ],
      "source": [
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "dcgan.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
