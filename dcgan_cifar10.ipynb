{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DCGAN with CIFTAR10"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/tensorflow/docs\n",
            "  Cloning https://github.com/tensorflow/docs to /private/var/folders/mc/_4lfk6ls089fwdc0qgw721cm0000gn/T/pip-req-build-ol75oxom\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/docs /private/var/folders/mc/_4lfk6ls089fwdc0qgw721cm0000gn/T/pip-req-build-ol75oxom\n",
            "  Resolved https://github.com/tensorflow/docs to commit 0c78d6d8ad6ced9178a081d41ead24ae80d88958\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: astor in /Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages (from tensorflow-docs==0.0.0.dev0) (0.8.1)\n",
            "Requirement already satisfied: absl-py in /Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages (from tensorflow-docs==0.0.0.dev0) (1.4.0)\n",
            "Requirement already satisfied: jinja2 in /Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages (from tensorflow-docs==0.0.0.dev0) (3.1.2)\n",
            "Requirement already satisfied: nbformat in /Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages (from tensorflow-docs==0.0.0.dev0) (5.8.0)\n",
            "Requirement already satisfied: protobuf>=3.12 in /Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages (from tensorflow-docs==0.0.0.dev0) (4.22.3)\n",
            "Requirement already satisfied: pyyaml in /Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages (from tensorflow-docs==0.0.0.dev0) (6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages (from jinja2->tensorflow-docs==0.0.0.dev0) (2.1.2)\n",
            "Requirement already satisfied: fastjsonschema in /Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (2.16.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (4.17.3)\n",
            "Requirement already satisfied: jupyter-core in /Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (5.3.0)\n",
            "Requirement already satisfied: traitlets>=5.1 in /Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (5.9.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==0.0.0.dev0) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==0.0.0.dev0) (0.19.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages (from jupyter-core->nbformat->tensorflow-docs==0.0.0.dev0) (3.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YfIk2es3hJEd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-24 13:36:54.494919: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import tensorflow_docs.vis.embed as embed\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from IPython import display"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S4PIDhoDLbsZ"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "EPOCHS = 5 # was 50\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "## Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a4fYMGxGhrna"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (_, _) = tf.keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Pre-Processing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reshape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NFC2ghIdiZYE"
      },
      "outputs": [],
      "source": [
        "#train_images = train_images.resize(64)\n",
        "train_images = train_images.reshape(train_images.shape[0], 32, 32, 3).astype('float32')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalize \n",
        "Images to [-1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_images = (train_images - 127.5) / 127.5  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch & shuffle "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-yKCCQOoJ7cn"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "THY-sZMiQ4UV"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-tEyxE-GMC48"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6bpTcDqoLWjY"
      },
      "outputs": [],
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(2*2*256, use_bias=False)) \n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((2, 2, 256)))\n",
        "    #assert model.output_shape == (None, 4, 4, 256)  # Note: None is the batch size \n",
        "    model.add(layers.Conv2DTranspose(128, 5, strides=2, padding='same', use_bias=False))\n",
        "    #assert model.output_shape == (None, 8, 8, 128) \n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, 5, strides=2, padding='same', use_bias=False)) \n",
        "    #assert model.output_shape == (None, 16, 16, 64) \n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(16, 5, strides=2, padding='same', use_bias=False)) \n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(32, 5, strides=2, padding='same', use_bias=False, activation='tanh')) \n",
        "    #assert model.output_shape == (None, 32, 32, 3) \n",
        "\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GyWgG09LCSJl"
      },
      "source": [
        "Generate an image without model training - ensure it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "gl7jcC7TdPTG",
        "outputId": "a222167f-4484-44ca-9106-dca0b5b867d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x14ffd0850>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "generator = make_generator_model()\n",
        "\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D0IKnaCtg6WE"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dw2tPLmk2pEP"
      },
      "outputs": [],
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(32, 5, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(64, 5, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(256, 5, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(3))\n",
        "    \n",
        "\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QhPneagzCaQv"
      },
      "source": [
        "Generate a decision without training - ensure it works.\n",
        "Positive values for real images, and negative values for fake images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDkA05NE6QMs",
        "outputId": "a162fb32-3501-42b4-ead9-4f059a9310d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([[-4.1820887e-05 -2.5532843e-04  6.9261593e-04]], shape=(1, 3), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "discriminator = make_discriminator_model()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "## Model Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "psQfmXxYKU3X"
      },
      "outputs": [],
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKY_iPSPNWoj"
      },
      "source": [
        "### Discriminator loss\n",
        "\n",
        "This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wkMNfBWlT-PV"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd-3GCUEiKtv"
      },
      "source": [
        "### Generator loss\n",
        "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, compare the discriminators decisions on the generated images to an array of 1s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "90BIcCKcDMxz"
      },
      "outputs": [],
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iWCn_PVdEJZ7"
      },
      "outputs": [],
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mWtinsGDPJlV"
      },
      "source": [
        "## Training Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CA1w-7s2POEy"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "## Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NS2GWywBbAWo"
      },
      "outputs": [],
      "source": [
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3t5ibNo05jCB"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2M7LmLtGEMQJ"
      },
      "outputs": [],
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RmdVsmvhPxyy"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dZrd4CdjR-Fp"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "Ly3UN0SLLY2l",
        "outputId": "d4a43ff2-fa60-4ce2-d1f5-ebdbfbaae37e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-24 13:37:00.114463: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [50000,32,32,3]\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-04-24 13:37:00.114713: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [50000,32,32,3]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/var/folders/mc/_4lfk6ls089fwdc0qgw721cm0000gn/T/ipykernel_13311/1513693288.py\", line 8, in train_step  *\n        real_output = discriminator(images, training=True)\n    File \"/Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages/keras/engine/input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_1' (type Sequential).\n    \n    Input 0 of layer \"conv2d\" is incompatible with the layer: expected axis -1 of input shape to have value 32, but received input with shape (256, 32, 32, 3)\n    \n    Call arguments received by layer 'sequential_1' (type Sequential):\n      • inputs=tf.Tensor(shape=(256, 32, 32, 3), dtype=float32)\n      • training=True\n      • mask=None\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(train_dataset, EPOCHS)\n",
            "Cell \u001b[0;32mIn[19], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m image_batch \u001b[39min\u001b[39;00m dataset:\n\u001b[0;32m----> 6\u001b[0m   train_step(image_batch)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Produce images for the GIF as you go\u001b[39;00m\n\u001b[1;32m      9\u001b[0m display\u001b[39m.\u001b[39mclear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m/var/folders/mc/_4lfk6ls089fwdc0qgw721cm0000gn/T/__autograph_generated_filebk0yjbyq.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m gen_tape, ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m disc_tape:\n\u001b[1;32m     10\u001b[0m     generated_images \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(generator), (ag__\u001b[39m.\u001b[39mld(noise),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)\n\u001b[0;32m---> 11\u001b[0m     real_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(discriminator), (ag__\u001b[39m.\u001b[39;49mld(images),), \u001b[39mdict\u001b[39;49m(training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), fscope)\n\u001b[1;32m     12\u001b[0m     fake_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(discriminator), (ag__\u001b[39m.\u001b[39mld(generated_images),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)\n\u001b[1;32m     13\u001b[0m     gen_loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(generator_loss), (ag__\u001b[39m.\u001b[39mld(fake_output),), \u001b[39mNone\u001b[39;00m, fscope)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.11/site-packages/keras/engine/input_spec.py:280\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    275\u001b[0m             value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mvalue\n\u001b[1;32m    276\u001b[0m         \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m shape_as_list[\u001b[39mint\u001b[39m(axis)] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m {\n\u001b[1;32m    277\u001b[0m             value,\n\u001b[1;32m    278\u001b[0m             \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m         }:\n\u001b[0;32m--> 280\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    282\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mincompatible with the layer: expected axis \u001b[39m\u001b[39m{\u001b[39;00maxis\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof input shape to have value \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mbut received input with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshape \u001b[39m\u001b[39m{\u001b[39;00mdisplay_shape(x\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m             )\n\u001b[1;32m    287\u001b[0m \u001b[39m# Check shape.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mshape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m shape\u001b[39m.\u001b[39mrank \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/var/folders/mc/_4lfk6ls089fwdc0qgw721cm0000gn/T/ipykernel_13311/1513693288.py\", line 8, in train_step  *\n        real_output = discriminator(images, training=True)\n    File \"/Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/mattklich/opt/anaconda3/envs/gan/lib/python3.11/site-packages/keras/engine/input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_1' (type Sequential).\n    \n    Input 0 of layer \"conv2d\" is incompatible with the layer: expected axis -1 of input shape to have value 32, but received input with shape (256, 32, 32, 3)\n    \n    Call arguments received by layer 'sequential_1' (type Sequential):\n      • inputs=tf.Tensor(shape=(256, 32, 32, 3), dtype=float32)\n      • training=True\n      • mask=None\n"
          ]
        }
      ],
      "source": [
        "train(train_dataset, EPOCHS)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Restore The Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhXsd0srPo8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.InitializationOnlyStatus at 0x16aeb0a90>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P4M_vIbUi7c0"
      },
      "source": [
        "## Create a GIF (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfO5wCdclHGL"
      },
      "outputs": [],
      "source": [
        "def display_image(epoch_no):\n",
        "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_image(EPOCHS)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NywiH3nL8guF"
      },
      "source": [
        "### Animate Image to GIF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embed.embed_file(anim_file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "dcgan.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
